---
title: "Practical session on supervised machine learning in R"
author: "Leonard Wee"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r install-packages, include=FALSE}
#note these are the packages you need to install and have ready for this practical exercise
#
boolPackagesPresent <- function(lop){
  #boolean list of which function in a given list "lop" that is already installed
  #returns a boolen iterand of which functions in "lop" are already present
  packages.present <- (lop %in% installed.packages()[,"Package"]) 
  return(packages.present)
}
installRequiredPackages <- function(lop){
  new.packages <- lop[!boolPackagesPresent(lop)] #note negation - new packages are what is NOT installed yet
  if(length(new.packages))
    install.packages(new.packages) #shortcut to run through and install the list without making explicit loop
  sapply(lop, require, character.only = TRUE) #simple "apply" is the really smart R way to loop over
  return(lop[boolPackagesPresent(lop)])
  #returns elements of "lop" that are now successfully installed
}

installRequiredPackages( c("dplyr",
                           "magrittr",
                           "caret", "pROC",
                           "ggplot2"))
```

## Step 1 : getting the data ready

Reminder that these are steps we have already seen before to load some data and recode it slightly. Then we are also going to drop some columns which we are intentionally not going to use.

```{r head-titanic-data, include=FALSE}
pathToData <- './' #edit this line

titanic3 <- read.csv2( file.path(pathToData,'titanic3.csv') )

titanic3 <- titanic3[!is.na(titanic3$survived),]

colnames(titanic3)[1] <- "pclass"

recodedTitanic <- dplyr::mutate(titanic3,
    #
    pclass = as.factor(pclass),
    #
    survived = recode_factor(survived,
                        "1" = "surv",
                        "0" = "died"),
    #
    sex = recode_factor(sex,
                        "male" = "m",
                        "female" = "f"),
    #
    age = floor(age),
    #
    fare = floor(fare),
    #
    embarked = recode_factor(embarked,
                             "S" = "South",
                             "Q" = "Queens",
                             "C" = "Cherb",
                             .default = "Unknown"),
    #
    cabin = gsub("^$","X",cabin),
    cabin = as.factor( substr(cabin,1,1) )
  )

recodedTitanic <- dplyr::select(recodedTitanic,
                                -name,
                                -ticket,
                                -boat,
                                -body,
                                -home.dest
  
)
```

## Step 2 : inspect missing value and try to impute

In supervised machine learning, we will have to be extra careful how we handle missing values. For sure, we cannot do supervised learning with the missing label survival label, so that has to drop. Then we should perhaps think about what to do with the missing ages.

```{r first summary}
summary(recodedTitanic)
```

There is only one missing value for fare, so we will impute it based on the median fare in the same passenger class.

```{r fare-imputation, include=F}
thisClass <- recodedTitanic$pclass[is.na(recodedTitanic$fare)]
thisSection <- dplyr::filter(recodedTitanic, pclass == thisClass)
medianFare = median(thisSection$fare, na.rm = T)
recodedTitanic$fare[is.na(recodedTitanic$fare)] <- as.numeric(medianFare)
```

There are quite a lot of missing ages, try to impute. **Be careful not to impute using the ground truth variable of survival!**

```{r mice-imputation, include=FALSE}
installRequiredPackages( c("VIM","mice") )
imputation <- dplyr::select(recodedTitanic, -survived)
temp <- mice(imputation, method = "pmm")
imputation <- complete(temp,1)
```

```{r check-imputation}
#check distribution
par(mfrow=c(1,2))
plot(imputation$age)
plot(recodedTitanic$age)
```

```{r last summary}
imputedTitanic <- imputation
imputedTitanic$survived <- recodedTitanic$survived
summary(imputedTitanic)
rm(imputation, temp, thisClass, thisSection, medianFare)
```

Now we can see that our data frame is complete and ready for machine learning.

## Step 3 : Logistic regression in machine learning style ("Elastic Net")

```{r split-titanic-dataset}
library(caret) #checks that we have it installed

set.seed(999) #if we want to get the same quasi random splits every time

#in machine learning we often divide up the data into a set that will train the machine model
#and then test it in a set that the machine model has not ever seen yet
inTrain <- createDataPartition(imputedTitanic$survived, p = 0.80, list = F)
titanic_train <- imputedTitanic[ inTrain, ]
titanic_test <- imputedTitanic[ -inTrain, ]
rm(inTrain)
```

```{r glmnet-titanic-model}
trainControl <- trainControl(
  method = "repeatedcv", #repeated internal cross-validation
  number = 10, #10-fold cross-validation
  repeats = 10, #10 repeats of each fold
  search = "random",
  summaryFunction = twoClassSummary,
  classProbs=TRUE
  )

model_glmnet <- train(
  survived ~ . , data = titanic_train,
  method = "glmnet",
  trControl = trainControl,
  verbose = FALSE,
  tuneLength = 5,
  metric = "ROC"
)

model_glmnet
```

```{r evaluate-glmnet-model}
probabilitiesSelf <- predict(model_glmnet, titanic_train, type="prob")
myroc <- roc(predictor = probabilitiesSelf$surv,
             response = titanic_train$survived,
             levels = c("surv","died"), smooth=F)

probabilitiesTest <- predict(model_glmnet, titanic_test, type="prob")
testroc <- roc(predictor = probabilitiesTest$surv,
             response = titanic_test$survived,
             levels = c("surv","died"), smooth=F)

par(mfrow=c(1,2))
plot.roc(myroc, legacy.axes = T, print.auc = T, col="green", print.thres = "best")
plot.roc(testroc, legacy.axes = T, print.auc = T, col="red")
```

## Step 4 : Tree-based classifier (Recursive Partitioning "RPART")

This is one of the simplest models to build and there are actually no tuning parameters to steer the machine learning.

```{r tree-titanic-model}
trainControl <- trainControl(
  method = "repeatedcv", #repeated internal cross-validation
  number = 10, #10-fold cross-validation
  repeats = 10, #10 repeats of each fold
  #search = "random",
  summaryFunction = twoClassSummary,
  classProbs=TRUE
  )

model_rpart <- train(
  survived ~ . , data = titanic_train,
  method = "rpart",
  trControl = trainControl,
  #verbose = FALSE,
  #tuneLength = 5
  metric = "ROC"
)

installRequiredPackages(c("rpart.plot"))
rpart.plot(model_rpart$finalModel)
```


```{r evaluate-tree-model}
probabilitiesSelf <- predict(model_rpart, titanic_train, type="prob")
myroc <- roc(predictor = probabilitiesSelf$surv,
             response = titanic_train$survived,
             levels = c("surv","died"), smooth=F)

probabilitiesTest <- predict(model_rpart, titanic_test, type="prob")
testroc <- roc(predictor = probabilitiesTest$surv,
             response = titanic_test$survived,
             levels = c("surv","died"), smooth=F)

par(mfrow=c(1,2))
plot.roc(myroc, legacy.axes = T, print.auc = T, col="green", print.thres = "best")
plot.roc(testroc, legacy.axes = T, print.auc = T, col="red")
```


## Step 5 : Random forest classifier ("rf")

The random forest classifier here is going to be based on an ensemble of 2000 trees, each with a randomly-picked subset of 5 out of the 8 available variables.

```{r rf-titanic-model}
trainControl <- trainControl(
  method = "repeatedcv", #repeated internal cross-validation
  number = 10, #10-fold cross-validation
  repeats = 10, #10 repeats of each fold
  #search = "random",
  summaryFunction = twoClassSummary,
  classProbs=TRUE,
  allowParallel=TRUE
  )

tgControl <- expand.grid(
  .mtry = 5
)

model_forest <- train(
  survived ~ . , data = titanic_train,
  method = "rf",
  trControl = trainControl,
  #verbose = FALSE,
  #tuneLength = 5
  metric = "ROC",
  tuneGrid = tgControl,
  num.trees = 2000
)

model_forest
```

```{r evaluate-forest-model}
probabilitiesSelf <- predict(model_forest, titanic_train, type="prob")
myroc <- roc(predictor = probabilitiesSelf$surv,
             response = titanic_train$survived,
             levels = c("surv","died"), smooth=F)

probabilitiesTest <- predict(model_forest, titanic_test, type="prob")
testroc <- roc(predictor = probabilitiesTest$surv,
             response = titanic_test$survived,
             levels = c("surv","died"), smooth=F)

par(mfrow=c(1,2))
plot.roc(myroc, legacy.axes = T, print.auc = T, col="green", print.thres = "best")
plot.roc(testroc, legacy.axes = T, print.auc = T, col="red")
```


## Step 6 : Support Vector Machine ("SVM")

For this, we have quite some choices to make, and tuning can be a bit involved. In this example, we will only use a simple multi-linear hyperplane descriptor to cut the high-dimensional space into survived and died  regions.

```{r svm-titanic-model}
trainControl <- trainControl(
  method = "cv", #internal K-FOLD cross-validation
  number = 10, #10-fold cross-validation
  #repeats = 10, #10 repeats of each fold
  search = "random",
  summaryFunction = twoClassSummary,
  classProbs=TRUE,
  allowParallel=TRUE
  )

#tgControl <- expand.grid(
#  .mtry = 5
#)

model_svm <- train(
  survived ~ . , data = titanic_train,
  method = "svmLinear",
  trControl = trainControl,
  #verbose = FALSE,
  tuneLength = 5,
  metric = "ROC"
)

model_svm
```


```{r evaluate-svm-model}
probabilitiesSelf <- predict(model_svm, titanic_train, type="prob")
myroc <- roc(predictor = probabilitiesSelf$surv,
             response = titanic_train$survived,
             levels = c("surv","died"), smooth=F)

probabilitiesTest <- predict(model_svm, titanic_test, type="prob")
testroc <- roc(predictor = probabilitiesTest$surv,
             response = titanic_test$survived,
             levels = c("surv","died"), smooth=F)

par(mfrow=c(1,2))
plot.roc(myroc, legacy.axes = T, print.auc = T, col="green", print.thres = "best")
plot.roc(testroc, legacy.axes = T, print.auc = T, col="red")
```


## Step 7 : Simple ANN

Generally speaking we would not do try to do Neural Nets in R (for this we would really need python and machines with very high parallel-processing power) but we might be able to give this a shot here.

```{r nnet-titanic-model}
trainControl <- trainControl(
  method = "cv", #internal K-FOLD cross-validation
  number = 10, #10-fold cross-validation
  #repeats = 10, #10 repeats of each fold
  search = "random",
  summaryFunction = twoClassSummary,
  classProbs=TRUE,
  allowParallel=TRUE
  )

#tgControl <- expand.grid(
#  .size = c(5, 10, 15, 20),
#  .decay = c(0.0001, 0.001, 0.01, 0.1, 1)
#)

model_nnet <- train(
  survived ~ . , data = titanic_train,
  method = "nnet",
  trControl = trainControl,
  verbose = FALSE,
  #tuneGrid = tgControl,
  tuneLength = 10,
  metric = "ROC"
)

model_nnet
```


```{r evaluate-nnet-model}
probabilitiesSelf <- predict(model_nnet, titanic_train, type="prob")
myroc <- roc(predictor = probabilitiesSelf$surv,
             response = titanic_train$survived,
             levels = c("surv","died"), smooth=F)

probabilitiesTest <- predict(model_nnet, titanic_test, type="prob")
testroc <- roc(predictor = probabilitiesTest$surv,
             response = titanic_test$survived,
             levels = c("surv","died"), smooth=F)

par(mfrow=c(1,2))
plot.roc(myroc, legacy.axes = T, print.auc = T, col="green", print.thres = "best")
plot.roc(testroc, legacy.axes = T, print.auc = T, col="red")
```

## step 8 : Compare the different models at the SAME operating threshold

Let's assume that the operating point we want is the probability threshold of 0.5.


```{r confusion-tree}
predictions_tree <- ifelse(
  predict(model_rpart, titanic_test, type="prob")$surv > 0.5, "surv", "died")
predictions_tree <- as.factor(predictions_tree)
confusionTREE <- confusionMatrix(predictions_tree, titanic_test$survived)
confusionTREE$table
#confusionTREE
```


```{r confusion-forest}
predictions_forest <- ifelse(
  predict(model_forest, titanic_test, type="prob")$surv > 0.5, "surv", "died")
predictions_forest <- as.factor(predictions_forest)
confusionFOREST <- confusionMatrix(predictions_forest, titanic_test$survived)
confusionFOREST$table
#confusionFOREST
```


```{r confusion-elastic-logit}
predictions_enet <- ifelse(
  predict(model_glmnet, titanic_test, type="prob")$surv > 0.5, "surv", "died")
predictions_enet <- as.factor(predictions_enet)
confusionENET <- confusionMatrix(predictions_enet, titanic_test$survived)
confusionENET$table
#confusionENET
```


```{r confusion-svm}
predictions_svm <- ifelse(
  predict(model_svm, titanic_test, type="prob")$surv > 0.5, "surv", "died")
predictions_svm <- as.factor(predictions_svm)
confusionSVM <- confusionMatrix(predictions_svm, titanic_test$survived)
confusionSVM$table
#confusionSVM
```

```{r confusion-nnet}
predictions_nnet <- ifelse(
  predict(model_nnet, titanic_test, type="prob")$surv > 0.5, "surv", "died")
predictions_nnet <- as.factor(predictions_nnet)
confusionNNET <- confusionMatrix(predictions_nnet, titanic_test$survived)
confusionNNET$table
#confusionNNET
```

